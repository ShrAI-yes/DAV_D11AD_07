{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IYsYXG2S0AO"
      },
      "source": [
        "# **Title: Perform the steps involved in Text Analytics in Python & R**\n",
        "\n",
        "Lab Objectives:\n",
        "* To introduce the concept of text analytics and its applications.\n",
        "\n",
        "Lab Outcome(LO): Implement various Regression techniques for prediction. (LO2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiQVUdnJTCci"
      },
      "source": [
        "## **Python**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rleky46YBXi",
        "outputId": "c438ee21-ee58-49be-e092-0b980b728cab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KMn6sK3jShUr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk import ne_chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9LMy7IOWOlm"
      },
      "source": [
        "## Load Dataset\n",
        "## **[Kaggle Dataset Link](https://www.kaggle.com/datasets/mujeebunissa/movies-csv)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pWEeUkWUGMk",
        "outputId": "97c73ffd-1b87-458f-d4e8-32792645a696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9166 entries, 0 to 9165\n",
            "Data columns (total 7 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Unnamed: 0    9166 non-null   int64  \n",
            " 1   id            9166 non-null   int64  \n",
            " 2   title         9166 non-null   object \n",
            " 3   overview      9165 non-null   object \n",
            " 4   popularity    9166 non-null   float64\n",
            " 5   vote_average  9166 non-null   float64\n",
            " 6   vote_count    9166 non-null   int64  \n",
            "dtypes: float64(2), int64(3), object(2)\n",
            "memory usage: 501.4+ KB\n",
            "None \n",
            "\n",
            " Non-Null Rows:\n",
            " Unnamed: 0      0\n",
            "id              0\n",
            "title           0\n",
            "overview        1\n",
            "popularity      0\n",
            "vote_average    0\n",
            "vote_count      0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Dataset/movies.csv')\n",
        "\n",
        "print(data.info(),'\\n\\n Non-Null Rows:\\n',data.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iOBKn2NqUVo_"
      },
      "outputs": [],
      "source": [
        "data.dropna(axis=0,inplace=True)\n",
        "\n",
        "text = data[['title','overview']]\n",
        "text = text.sample(frac=1,random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ExqjM4GoZEnF"
      },
      "outputs": [],
      "source": [
        "sample = text.sample(n=1)\n",
        "para = sample['overview'].values[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW2Lt5inew7_"
      },
      "source": [
        "### Word Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JiMwUOeV6qD",
        "outputId": "9f9b87f1-3cba-46e1-9c90-90ff20239b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['An', 'opulent', 'beach', 'resort', 'provides', 'a', 'scenic', 'background', 'to', 'this', 'amusing', 'whodunit', 'as', 'Poirot', 'attempts', 'to', 'uncover', 'the', 'nefarious', 'evildoer', 'behind', 'the', 'strangling', 'of', 'a', 'notorious', 'stage', 'star', '.']\n"
          ]
        }
      ],
      "source": [
        "token = word_tokenize(contractions.fix(para))\n",
        "print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1YFMVI8e-PL"
      },
      "source": [
        "### Sentence Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3PNHvtvfDwS",
        "outputId": "13acc4ee-fe4a-46dc-dd89-b70ed770f69b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An opulent beach resort provides a scenic background to this amusing whodunit as Poirot attempts to uncover the nefarious evildoer behind the strangling of a notorious stage star. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence = sent_tokenize(para)\n",
        "for sent in sentence:\n",
        "  print(sent,'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggjNXVaoyRSp"
      },
      "source": [
        "### Word Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYOkp-3oyRAj",
        "outputId": "a486c35b-3d6b-4aab-aa2f-1c31f7fecd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 Most words in the text:  [('a', 2), ('to', 2), ('the', 2), ('An', 1), ('opulent', 1)]\n"
          ]
        }
      ],
      "source": [
        "frequency = FreqDist(token)\n",
        "print('5 Most words in the text: ',frequency.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb7K4XXDgmXk"
      },
      "source": [
        "### Punctuation and Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9su3S2vguQM",
        "outputId": "d7af5a2a-8a13-4295-d1dd-7dee8c80ba85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "opulent beach resort provides scenic background amusing whodunit Poirot attempts uncover nefarious evildoer behind strangling notorious stage star\n"
          ]
        }
      ],
      "source": [
        "stopword = stopwords.words('english')\n",
        "punct = list(string.punctuation)\n",
        "\n",
        "def text_clean(tokens):\n",
        "  cleaned = []\n",
        "  for word in tokens:\n",
        "    if word.lower() not in stopword and word not in punct:\n",
        "      cleaned.append(word)\n",
        "  return cleaned\n",
        "\n",
        "cleaned_tokens = text_clean(token)\n",
        "cleaned_para = ' '.join(cleaned_tokens)\n",
        "print(cleaned_para)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzqKpZKznK4S"
      },
      "source": [
        "### Lexicon Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QnoNfbp0nSSh"
      },
      "outputs": [],
      "source": [
        "stem = PorterStemmer()\n",
        "lemmatize = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdh92kvVonKO",
        "outputId": "f64f4fce-db17-43e2-906a-28842f800e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Sentence:  opul beach resort provid scenic background amus whodunit poirot attempt uncov nefari evildo behind strangl notori stage star\n",
            "Lemmatized Sentence:  opulent beach resort provide scenic background amuse whodunit Poirot attempt uncover nefarious evildoer behind strangle notorious stage star\n"
          ]
        }
      ],
      "source": [
        "stemmed_tokens = [stem.stem(word) for word in cleaned_tokens]\n",
        "stemmed_para = ' '.join(stemmed_tokens)\n",
        "print('Stemmed Sentence: ',stemmed_para)\n",
        "\n",
        "lemmatized_tokens = [lemmatize.lemmatize(word, pos='v') for word in cleaned_tokens]\n",
        "lemmatized_para = ' '.join(lemmatized_tokens)\n",
        "print('Lemmatized Sentence: ',lemmatized_para)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Tcc-4wq1FY"
      },
      "source": [
        "### POS Tagging\n",
        "\n",
        "* CC: Coordinating conjunction\n",
        "* CD: Cardinal number\n",
        "* DT: Determiner\n",
        "* EX: Existential there\n",
        "* FW: Foreign word\n",
        "* IN: Preposition or subordinating conjunction\n",
        "* JJ: Adjective\n",
        "* JJR: Adjective, comparative\n",
        "* JJS: Adjective, superlative\n",
        "* LS: List item marker\n",
        "* MD: Modal\n",
        "* NN: Noun, singular or mass\n",
        "* NNS: Noun, plural\n",
        "* NNP: Proper noun, singular\n",
        "* NNPS: Proper noun, plural\n",
        "* PDT: Predeterminer\n",
        "* POS: Possessive ending\n",
        "* PRP: Personal pronoun\n",
        "* RB: Adverb\n",
        "* RBR: Adverb, comparative\n",
        "* RBS: Adverb, superlative\n",
        "* RP: Particle\n",
        "* SYM: Symbol\n",
        "* TO: to\n",
        "* UH: Interjection\n",
        "* VB: Verb, base form\n",
        "* VBD: Verb, past tense\n",
        "* VBG: Verb, gerund or present participle\n",
        "* VBN: Verb, past participle\n",
        "* VBP: Verb, non-3rd person singular present\n",
        "* VBZ: Verb, 3rd person singular present\n",
        "* WDT: Wh-determiner\n",
        "* WP: Wh-pronoun\n",
        "* WP$: Possessive wh-pronoun\n",
        "* WRB: Wh-adverb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z-MtZn8q5t-",
        "outputId": "2b7afe21-9df2-4864-ca35-674056c076ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech:  <function pos_tag at 0x7c81785bed40>\n"
          ]
        }
      ],
      "source": [
        "pos_tagged = pos_tag(token)\n",
        "print('Parts of Speech: ', pos_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkOizXC3sdlC"
      },
      "source": [
        "### Named Entity Recognization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzXnXFdavNYY",
        "outputId": "b4490c78-8d38-4e98-e98b-f17395fcb3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Poirot']\n"
          ]
        }
      ],
      "source": [
        "ner_tagged = nltk.ne_chunk(pos_tagged)\n",
        "\n",
        "names = []\n",
        "for chunk in ner_tagged:\n",
        "    if isinstance(chunk, nltk.Tree):\n",
        "        names.append(\" \".join([token for token, pos in chunk.leaves()]))\n",
        "\n",
        "print(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO2v4O6xzc4n"
      },
      "source": [
        "### Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDwAoBOyzweb",
        "outputId": "2f0500ae-faba-4d2c-c8a8-5825e2e051fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "P73VxnpTz3yR"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from requests import HTTPError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0deRgd6Oz6Hu"
      },
      "outputs": [],
      "source": [
        "url = 'https://github.com/LifnaJos/ADL601-Data-Analytics-and-Visualization-Lab/blob/main/Experiments/Experiment_7.md'\n",
        "\n",
        "try:\n",
        "  response = requests.get(url)\n",
        "except HTTPError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "whlRqvg_0WcA"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(response.text, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl_c3q1I0iLz",
        "outputId": "ecdf55db-11c6-406a-d9fb-f3329501809c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\\"#experiment---7-perform-the-steps-involved-in-text-analytics-in-python--r\\\"\n",
            "\\\"#lab-outcomes-lo\\\"\n",
            "\\\"#task-to-be-performed-\\\"\n",
            "\\\"#tools--libraries-to-be-explored\\\"\n",
            "\\\"#theory-to-be-written\\\"\n",
            "\\\"#outcome-\\\"\n",
            "\\\"#online-resources\\\"\n",
            "\\\"https://github.com/LifnaJos/ADC601-Data-Analytics-Visualization/blob/DAV_Colab_Notebooks/Data_Preprocessing_techniques.ipynb\\\"\n",
            "\\\"https://guides.library.upenn.edu/penntdm/python\\\"\n",
            "\\\"https://machinelearninggeek.com/text-analytics-for-beginners-using-python-nltk/\\\"\n",
            "\\\"https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\\\"\n",
            "\\\"https://www.kdnuggets.com/2020/05/text-mining-python-steps-examples.html\\\"\n",
            "\\\"https://www.youtube.com/watch?v=bZoC-UW50sI&list=PLH6mU1kedUy-xjgiuvqMkVn8npK0TGAv5\\\"\n",
            "\\\"https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/\\\"\n"
          ]
        }
      ],
      "source": [
        "links = soup.find_all('a')\n",
        "\n",
        "for link in links:\n",
        "    print(link.get('href'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoU4lQyE1yy3"
      },
      "source": [
        "## **R**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lX2ES16E2-Wx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5007026-b257-4452-ddce-4747c6834a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘slam’, ‘BH’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘RcppTOML’, ‘here’, ‘png’, ‘reticulate’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages(\"tokenizers\")\n",
        "install.packages(\"tm\")\n",
        "install.packages(\"udpipe\")\n",
        "install.packages(\"spacyr\")\n",
        "install.packages(\"rvest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eAh8zMZs5qle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbcaf0b-5334-467c-a3a1-26c1bba6e1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: NLP\n",
            "\n"
          ]
        }
      ],
      "source": [
        "library(tokenizers)\n",
        "library(tm)\n",
        "library(udpipe)\n",
        "library(spacyr)\n",
        "library(rvest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jayn4x5O4Cjy"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnuCb0DB2yU1",
        "outputId": "e94ff57c-ce73-45fd-a13c-ad5b1f638b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentence Tokens:\"\n",
            "[[1]]\n",
            "[1] \"Suave, charming and volatile, Reggie Kray and his unstable twin brother Ronnie start to leave their mark on the London underworld in the 1960s.\" \n",
            "[2] \"Using violence to get what they want, the siblings orchestrate robberies and murders while running nightclubs and protection rackets.\"           \n",
            "[3] \"With police Detective Leonard \\\"Nipper\\\" Read hot on their heels, the brothers continue their rapid rise to power and achieve tabloid notoriety.\"\n",
            "\n",
            "[1] \"Word Tokens:\"\n",
            "[[1]]\n",
            " [1] \"suave\"       \"charming\"    \"and\"         \"volatile\"    \"reggie\"     \n",
            " [6] \"kray\"        \"and\"         \"his\"         \"unstable\"    \"twin\"       \n",
            "[11] \"brother\"     \"ronnie\"      \"start\"       \"to\"          \"leave\"      \n",
            "[16] \"their\"       \"mark\"        \"on\"          \"the\"         \"london\"     \n",
            "[21] \"underworld\"  \"in\"          \"the\"         \"1960s\"       \"using\"      \n",
            "[26] \"violence\"    \"to\"          \"get\"         \"what\"        \"they\"       \n",
            "[31] \"want\"        \"the\"         \"siblings\"    \"orchestrate\" \"robberies\"  \n",
            "[36] \"and\"         \"murders\"     \"while\"       \"running\"     \"nightclubs\" \n",
            "[41] \"and\"         \"protection\"  \"rackets\"     \"with\"        \"police\"     \n",
            "[46] \"detective\"   \"leonard\"     \"nipper\"      \"read\"        \"hot\"        \n",
            "[51] \"on\"          \"their\"       \"heels\"       \"the\"         \"brothers\"   \n",
            "[56] \"continue\"    \"their\"       \"rapid\"       \"rise\"        \"to\"         \n",
            "[61] \"power\"       \"and\"         \"achieve\"     \"tabloid\"     \"notoriety\"  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "text <- 'Suave, charming and volatile, Reggie Kray and his unstable twin brother Ronnie start to leave their mark on the London underworld in the 1960s. Using violence to get what they want, the siblings orchestrate robberies and murders while running nightclubs and protection rackets. With police Detective Leonard \"Nipper\" Read hot on their heels, the brothers continue their rapid rise to power and achieve tabloid notoriety.'\n",
        "\n",
        "word_tokens <- tokenize_words(text)\n",
        "sent_tokens <- tokenize_sentences(text)\n",
        "\n",
        "print(\"Sentence Tokens:\")\n",
        "print(sent_tokens)\n",
        "print(\"Word Tokens:\")\n",
        "print(word_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waaKKVbN4ETd"
      },
      "source": [
        "### Word Frequency Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNEgqr1A4Ltl",
        "outputId": "38e04d2f-a92a-409e-f916-313a9f39dc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Word Frequency Distribution:\"\n",
            "word_tokens\n",
            "      1960s     achieve         and     brother    brothers    charming \n",
            "          1           1           5           1           1           1 \n",
            "   continue   detective         get       heels         his         hot \n",
            "          1           1           1           1           1           1 \n",
            "         in        kray       leave     leonard      london        mark \n",
            "          1           1           1           1           1           1 \n",
            "    murders  nightclubs      nipper   notoriety          on orchestrate \n",
            "          1           1           1           1           2           1 \n",
            "     police       power  protection     rackets       rapid        read \n",
            "          1           1           1           1           1           1 \n",
            "     reggie        rise   robberies      ronnie     running    siblings \n",
            "          1           1           1           1           1           1 \n",
            "      start       suave     tabloid         the       their        they \n",
            "          1           1           1           4           3           1 \n",
            "         to        twin  underworld    unstable       using    violence \n",
            "          3           1           1           1           1           1 \n",
            "   volatile        want        what       while        with \n",
            "          1           1           1           1           1 \n"
          ]
        }
      ],
      "source": [
        "word_freq <- table(word_tokens)\n",
        "\n",
        "print(\"Word Frequency Distribution:\")\n",
        "print(word_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l187bVBh4oWb",
        "outputId": "addc5d0c-3cd3-464f-d7b6-2c5e032fecb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Top 5 most used words:\"\n",
            "word_tokens\n",
            "  and   the their    to    on \n",
            "    5     4     3     3     2 \n"
          ]
        }
      ],
      "source": [
        "sorted_freq <- sort(word_freq, decreasing = TRUE)\n",
        "\n",
        "print(\"Top 5 most used words:\")\n",
        "print(sorted_freq[1:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZtOVwmV4tLq"
      },
      "source": [
        "### Punctuation and Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KbVU8Ix5XgV",
        "outputId": "53413c60-a28f-4521-e3d9-624b2b211485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, content_transformer(tolower)):\n",
            "“transformation drops documents”\n",
            "Warning message in tm_map.SimpleCorpus(corpus, removePunctuation):\n",
            "“transformation drops documents”\n",
            "Warning message in tm_map.SimpleCorpus(corpus, removeWords, stopwords(\"english\")):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"suave charming  volatile reggie kray   unstable twin brother ronnie start  leave  mark   london underworld   1960s using violence  get   want  siblings orchestrate robberies  murders  running nightclubs  protection rackets  police detective leonard nipper read hot   heels  brothers continue  rapid rise  power  achieve tabloid notoriety\"\n"
          ]
        }
      ],
      "source": [
        "corpus <- Corpus(VectorSource(text))\n",
        "\n",
        "corpus <- tm_map(corpus, content_transformer(tolower))\n",
        "corpus <- tm_map(corpus, removePunctuation)\n",
        "corpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\n",
        "\n",
        "cleaned_text <- sapply(corpus, as.character)\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaegUikw6Qu3"
      },
      "source": [
        "### Lexicon Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMCuYHoS7cv3",
        "outputId": "1af36ef2-d555-45ef-eb8b-35623db51068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading udpipe model from https://raw.githubusercontent.com/jwijffels/udpipe.models.ud.2.5/master/inst/udpipe-ud-2.5-191206/english-ewt-ud-2.5-191206.udpipe to /content/english-ewt-ud-2.5-191206.udpipe\n",
            "\n",
            " - This model has been trained on version 2.5 of data from https://universaldependencies.org\n",
            "\n",
            " - The model is distributed under the CC-BY-SA-NC license: https://creativecommons.org/licenses/by-nc-sa/4.0\n",
            "\n",
            " - Visit https://github.com/jwijffels/udpipe.models.ud.2.5 for model license details.\n",
            "\n",
            " - For a list of all models and their licenses (most models you can download with this package have either a CC-BY-SA or a CC-BY-SA-NC license) read the documentation at ?udpipe_download_model. For building your own models: visit the documentation by typing vignette('udpipe-train', package = 'udpipe')\n",
            "\n",
            "Downloading finished, model stored at '/content/english-ewt-ud-2.5-191206.udpipe'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ud_model <- udpipe_download_model(language = \"english\")\n",
        "ud_model <- udpipe_load_model(ud_model$file_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm3WqtMm6UKP",
        "outputId": "1180d020-f061-41d4-b1ac-9288c6478bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas: Suave , charming and volatile , Reggie Kray and he unstable twin brother Ronnie start to leave they mark on the London underworld in the 1960 . use violence to get what they want , the sibling orchestrate robbery and murder while run nightclub and protection racket . with police detective Leonard \" Nipper \" read hot on they heel , the brother continue they rapid rise to power and achieve tabloid notoriety . \n",
            "Stems: Suave , charming and volatile , Reggie Kray and his unstable twin brother Ronnie start to leave their mark on the London underworld in the 1960s . Using violence to get what they want , the siblings orchestrate robberies and murders while running nightclubs and protection rackets . With police Detective Leonard \" Nipper \" Read hot on their heels , the brothers continue their rapid rise to power and achieve tabloid notoriety . \n"
          ]
        }
      ],
      "source": [
        "tokens <- udpipe_annotate(ud_model, x = text)\n",
        "\n",
        "lemmas <- as.data.frame(tokens)$lemma\n",
        "cat(\"Lemmas:\", lemmas, \"\\n\")\n",
        "\n",
        "stems <- as.data.frame(tokens)$token\n",
        "cat(\"Stems:\", stems, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1Nuxd2l8Gqi"
      },
      "source": [
        "### Part of Speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdvrakS18Kuw",
        "outputId": "4cd011c9-0238-4c38-c09e-fe41d6e79693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags: PROPN PUNCT NOUN CCONJ NOUN PUNCT PROPN PROPN CCONJ PRON ADJ NOUN NOUN PROPN VERB PART VERB PRON NOUN ADP DET PROPN NOUN ADP DET NOUN PUNCT VERB NOUN PART VERB PRON PRON VERB PUNCT DET NOUN ADJ NOUN CCONJ NOUN SCONJ VERB NOUN CCONJ NOUN NOUN PUNCT ADP NOUN PROPN PROPN PUNCT PROPN PUNCT VERB ADJ ADP PRON NOUN PUNCT DET NOUN VERB PRON ADJ NOUN ADP NOUN CCONJ NOUN NOUN NOUN PUNCT \n"
          ]
        }
      ],
      "source": [
        "pos_tags <- as.data.frame(tokens)$upos\n",
        "cat(\"POS tags:\", pos_tags, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qukm8Rc0968r"
      },
      "source": [
        "### Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vfLg5J7r99cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8beec089-a28b-4f5d-988c-d0fe25d79e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [1] \"\\\"Web scraping\\\"\"                                                                                                                   \n",
            " [2] \"news\"                                                                                                                               \n",
            " [3] \"newspapers\"                                                                                                                         \n",
            " [4] \"books\"                                                                                                                              \n",
            " [5] \"scholar\"                                                                                                                            \n",
            " [6] \"JSTOR\"                                                                                                                              \n",
            " [7] \"improve this section\"                                                                                                               \n",
            " [8] \"\\\"SASSCAL WebSAPI: A Web Scraping Application Programming Interface to Support Access to SASSCAL's Weather Data\\\"\"                  \n",
            " [9] \"10.5334/dsj-2021-024\"                                                                                                               \n",
            "[10] \"1683-1470\"                                                                                                                          \n",
            "[11] \"237719804\"                                                                                                                          \n",
            "[12] \"\\\"Search Engine History.com\\\"\"                                                                                                      \n",
            "[13] \"\\\"Joint optimization of wrapper generation and template detection\\\"\"                                                                \n",
            "[14] \"10.1145/1281192.1281287\"                                                                                                            \n",
            "[15] \"833565\"                                                                                                                             \n",
            "[16] \"the original\"                                                                                                                       \n",
            "[17] \"Semantic annotation based web scraping\"                                                                                             \n",
            "[18] \"\\\"Diffbot Is Using Computer Vision to Reinvent the Semantic Web\\\"\"                                                                  \n",
            "[19] \"\\\"TUTORIAL: AI research without coding: The art of fighting without fighting: Data science for qualitative researchers\\\"\"           \n",
            "[20] \"10.1016/j.jbusres.2020.06.012\"                                                                                                      \n",
            "[21] \"0148-2963\"                                                                                                                          \n",
            "[22] \"\\\"FAQ about linking – Are website terms of use binding contracts?\\\"\"                                                                \n",
            "[23] \"the original\"                                                                                                                       \n",
            "[24] \"\\\"Symbiotic Relationships: Pragmatic Acceptance of Data Scraping\\\"\"                                                                 \n",
            "[25] \"10.15779/Z38B39B\"                                                                                                                   \n",
            "[26] \"1086-3818\"                                                                                                                          \n",
            "[27] \"\\\"Internet Law, Ch. 06: Trespass to Chattels\\\"\"                                                                                     \n",
            "[28] \"\\\"What are the \\\"trespass to chattels\\\" claims some companies or website owners have brought?\\\"\"                                    \n",
            "[29] \"the original\"                                                                                                                       \n",
            "[30] \"\\\"Ticketmaster Corp. v. Tickets.com, Inc\\\"\"                                                                                         \n",
            "[31] \"\\\"American Airlines v. FareChase\\\"\"                                                                                                 \n",
            "[32] \"the original\"                                                                                                                       \n",
            "[33] \"\\\"American Airlines, FareChase Settle Suit\\\"\"                                                                                       \n",
            "[34] \"the original\"                                                                                                                       \n",
            "[35] \"Detecting and Blocking Site Scraping Attacks\"                                                                                       \n",
            "[36] \"\\\"Controversy Surrounds 'Screen Scrapers': Software Helps Users Access Web Sites But Activity by Competitors Comes Under Scrutiny\\\"\"\n",
            "[37] \"the original\"                                                                                                                       \n",
            "[38] \"\\\"QVC Inc. v. Resultly LLC, No. 14-06714 (E.D. Pa. filed Nov. 24, 2014)\\\"\"                                                          \n",
            "[39] \"the original\"                                                                                                                       \n",
            "[40] \"\\\"QVC Inc. v. Resultly LLC, No. 14-06714 (E.D. Pa. filed Nov. 24, 2014)\\\"\"                                                          \n",
            "[41] \"\\\"QVC Sues Shopping App for Web Scraping That Allegedly Triggered Site Outage\\\"\"                                                    \n",
            "[42] \"\\\"Did Iqbal/Twombly Raise the Bar for Browsewrap Claims?\\\"\"                                                                         \n",
            "[43] \"the original\"                                                                                                                       \n",
            "[44] \"\\\"Can Scraping Non-Infringing Content Become Copyright Infringement... Because Of How Scrapers Work? | Techdirt\\\"\"                  \n",
            "[45] \"\\\"Facebook v. Power Ventures\\\"\"                                                                                                     \n",
            "[46] \"\\\"UDSKRIFT AF SØ- & HANDELSRETTENS DOMBOG\\\"\"                                                                                        \n",
            "[47] \"the original\"                                                                                                                       \n",
            "[48] \"\\\"High Court of Ireland Decisions >> Ryanair Ltd -v- Billigfluege.de GMBH 2010 IEHC 47 (26 February 2010)\\\"\"                        \n",
            "[49] \"\\\"Intellectual Property: Website Terms of Use\\\"\"                                                                                    \n",
            "[50] \"the original\"                                                                                                                       \n",
            "[51] \"\\\"La réutilisation des données publiquement accessibles en ligne à des fins de démarchage commercial | CNIL\\\"\"                      \n",
            "[52] \"\\\"Can You Still Perform Web Scraping With The New CNIL Guidelines?\\\"\"                                                               \n",
            "[53] \"\\\"Spam Act 2003: An overview for business\\\"\"                                                                                        \n",
            "[54] \"the original\"                                                                                                                       \n",
            "[55] \"\\\"Spam Act 2003: A practical guide for business\\\"\"                                                                                  \n",
            "[56] \"Breaking Fraud & Bot Detection Solutions\"                                                                                           \n"
          ]
        }
      ],
      "source": [
        "url <- \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
        "webpage <- read_html(url)\n",
        "\n",
        "article_titles <- webpage %>%\n",
        "  html_nodes(\".text\") %>%\n",
        "  html_text()\n",
        "\n",
        "print(article_titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "We have successfully performed text analysis using different libraries in Python and R such as NLTK, spaCy, tm, tidytext, and udpipe. These libraries offer robust and flexible tools for various text analysis tasks such as tokenization, lexicon normalization, and named entity recognition."
      ],
      "metadata": {
        "id": "g5M0cAhuZU1S"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}